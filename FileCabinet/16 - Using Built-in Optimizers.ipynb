{"cells":[{"cell_type":"code","execution_count":1,"id":"dbee8e78-82c6-4d06-8f68-9db9502766b5","metadata":{"tags":[],"id":"dbee8e78-82c6-4d06-8f68-9db9502766b5","executionInfo":{"status":"ok","timestamp":1698009714797,"user_tz":420,"elapsed":6548,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}}},"outputs":[],"source":["# Import the torch module\n","import torch\n","import torch.optim as optim"]},{"cell_type":"markdown","id":"1b30fd12-211e-4ddd-a097-4ef7145d1f46","metadata":{"tags":[],"id":"1b30fd12-211e-4ddd-a097-4ef7145d1f46"},"source":["# torch.optim\n","\n","The torch module has an **optim** submodule where we can find classes implementing different optimization algorithms.\n","\n","> - Every optimizer constructor takes a list of parameters as the first input. All parameters passed to the optimizer are retained inside the optimizer object so the optimizer can update their values and access their grad attribute\n","> - Each optimizer exposes two methods: zero_grad and step. zero_grad zeroes the grad attribute of all the parameters passed to the optimizer upon construction."]},{"cell_type":"code","execution_count":2,"id":"7a35041d-dbd4-4bff-a0f8-141190b6607c","metadata":{"tags":[],"id":"7a35041d-dbd4-4bff-a0f8-141190b6607c","outputId":"c8e87908-a039-46a9-9402-b03d6486c000","executionInfo":{"status":"ok","timestamp":1698009714797,"user_tz":420,"elapsed":5,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ASGD',\n"," 'Adadelta',\n"," 'Adagrad',\n"," 'Adam',\n"," 'AdamW',\n"," 'Adamax',\n"," 'LBFGS',\n"," 'NAdam',\n"," 'Optimizer',\n"," 'RAdam',\n"," 'RMSprop',\n"," 'Rprop',\n"," 'SGD',\n"," 'SparseAdam',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '_functional',\n"," '_multi_tensor',\n"," 'lr_scheduler',\n"," 'swa_utils']"]},"metadata":{},"execution_count":2}],"source":["dir(optim)"]},{"cell_type":"markdown","id":"45fda6e6-d7df-4413-a614-bea63e0f2b2b","metadata":{"id":"45fda6e6-d7df-4413-a614-bea63e0f2b2b"},"source":["## Getting data and Defining model function and loss function"]},{"cell_type":"code","execution_count":3,"id":"e7824ac2-23a3-49e3-b647-f73a2b506b62","metadata":{"id":"e7824ac2-23a3-49e3-b647-f73a2b506b62","executionInfo":{"status":"ok","timestamp":1698009714797,"user_tz":420,"elapsed":3,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}}},"outputs":[],"source":["# temperature data in good old Celsius and measurements from a new thermometer.\n","t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0,3.0, -4.0, 6.0, 13.0, 21.0])\n","t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,33.9, 21.8, 48.4, 60.4, 68.4])\n","t_un = 0.1 * t_u  # feature scaling\n","\n","# linear model function\n","def model(t_u, w, b):\n","    return w * t_u + b\n","\n","# mean square error as loss function\n","def loss_fn(t_p, t_c):\n","    squared_diffs = (t_p - t_c)**2\n","    return squared_diffs.mean()"]},{"cell_type":"markdown","id":"ccc07458-6659-493c-9683-27159a5a93db","metadata":{"id":"ccc07458-6659-493c-9683-27159a5a93db"},"source":["## Defining Training Loop Using Optimizer and Autograd"]},{"cell_type":"code","execution_count":4,"id":"9ecdb5a0-9882-4d7e-a63e-2fac14caf0d3","metadata":{"tags":[],"id":"9ecdb5a0-9882-4d7e-a63e-2fac14caf0d3","executionInfo":{"status":"ok","timestamp":1698009714797,"user_tz":420,"elapsed":3,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}}},"outputs":[],"source":["# training loop\n","def training_loop(n_epochs, optimizer, params, t_u, t_c):\n","    for epoch in range(1, n_epochs + 1):\n","        t_p = model(t_u, *params)\n","        loss = loss_fn(t_p, t_c)\n","\n","        optimizer.zero_grad()  # zeroes the grad attribute of all the parameters passed to the optimizer upon construction\n","        loss.backward()\n","        optimizer.step()  # Performs a single optimization step (parameter update).\n","\n","        if epoch % 500 == 0:\n","            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n","\n","    return params"]},{"cell_type":"markdown","id":"2e5ca6ba-07d8-4bdb-8ad7-5298a14a14a3","metadata":{"id":"2e5ca6ba-07d8-4bdb-8ad7-5298a14a14a3"},"source":["## Using stochastic gradient descent (SGD)\n","The term stochastic comes from the fact that the gradient is typically obtained by averaging over a random subset of all input samples, called a minibatch."]},{"cell_type":"code","execution_count":5,"id":"b24f5c07-c1f6-43c2-a1e9-5cb88620740d","metadata":{"tags":[],"id":"b24f5c07-c1f6-43c2-a1e9-5cb88620740d","outputId":"b0199a04-4c2c-4847-a8fc-19a6562224ae","executionInfo":{"status":"ok","timestamp":1698009717778,"user_tz":420,"elapsed":2983,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 500, Loss 7.860120\n","Epoch 1000, Loss 3.828538\n","Epoch 1500, Loss 3.092191\n","Epoch 2000, Loss 2.957698\n","Epoch 2500, Loss 2.933134\n","Epoch 3000, Loss 2.928648\n","Epoch 3500, Loss 2.927830\n","Epoch 4000, Loss 2.927679\n","Epoch 4500, Loss 2.927652\n","Epoch 5000, Loss 2.927647\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([  5.3671, -17.3012], requires_grad=True)"]},"metadata":{},"execution_count":5}],"source":["# Letâ€™s create params and instantiate a gradient descent optimizer\n","params = torch.tensor([1.0, 0.0], requires_grad=True)\n","learning_rate = 1e-2\n","optimizer = optim.SGD([params], lr=learning_rate) # <1>\n","\n","training_loop(n_epochs = 5000,  optimizer = optimizer, params = params, t_u = t_un, t_c = t_c)"]},{"cell_type":"markdown","id":"5584918f-f312-4612-b088-44cd0d78e279","metadata":{"id":"5584918f-f312-4612-b088-44cd0d78e279"},"source":["# Examples of Using Adam Optimizer"]},{"cell_type":"code","execution_count":6,"id":"9a0383d4-a327-4f75-b235-63bc48338ac8","metadata":{"tags":[],"id":"9a0383d4-a327-4f75-b235-63bc48338ac8","outputId":"4de61036-39c1-467e-a8df-389b1e7eb83d","executionInfo":{"status":"ok","timestamp":1698009720303,"user_tz":420,"elapsed":2526,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 500, Loss 7.612900\n","Epoch 1000, Loss 3.086698\n","Epoch 1500, Loss 2.928578\n","Epoch 2000, Loss 2.927646\n","Epoch 2500, Loss 2.927645\n","Epoch 3000, Loss 2.927646\n","Epoch 3500, Loss 2.927645\n","Epoch 4000, Loss 2.927646\n","Epoch 4500, Loss 2.927646\n","Epoch 5000, Loss 2.927645\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([  0.5368, -17.3048], requires_grad=True)"]},"metadata":{},"execution_count":6}],"source":["params = torch.tensor([1.0, 0.0], requires_grad=True)\n","learning_rate = 1e-1\n","optimizer = optim.Adam([params], lr=learning_rate) # using adam\n","\n","training_loop(n_epochs = 5000, optimizer = optimizer,params = params,t_u = t_u, t_c = t_c)"]},{"cell_type":"code","execution_count":6,"id":"4340ec86-ae2c-418a-b2f8-e66bb155ae9b","metadata":{"id":"4340ec86-ae2c-418a-b2f8-e66bb155ae9b","executionInfo":{"status":"ok","timestamp":1698009720304,"user_tz":420,"elapsed":3,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}