{"cells":[{"cell_type":"code","execution_count":1,"id":"69cf90eb-36fb-4f22-9f36-4844f99d25c5","metadata":{"id":"69cf90eb-36fb-4f22-9f36-4844f99d25c5","executionInfo":{"status":"ok","timestamp":1698012799535,"user_tz":420,"elapsed":4828,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}}},"outputs":[],"source":["# Import the torch modules\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn"]},{"cell_type":"markdown","id":"0715fee5-5608-4405-941b-345030dd1054","metadata":{"id":"0715fee5-5608-4405-941b-345030dd1054"},"source":["# Subclassing nn.Module\n","\n","Another way to build models is to **subclass nn.Module**, which can do more complex things than nn.Sequential which just applies one layer after another.\n","\n","In order to **subclass nn.Module**, at a minimum we **need to define a forward function that takes the inputs to the module and returns the output**. This is where we define our moduleâ€™s computation.\n","\n","Note: With PyTorch, if we use standard torch operations, autograd will take care of the backward pass automatically; and indeed, an nn.Module never comes with a backward"]},{"cell_type":"code","execution_count":2,"id":"34a89c87-46d3-4f95-a2cf-65f332101640","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34a89c87-46d3-4f95-a2cf-65f332101640","executionInfo":{"status":"ok","timestamp":1698012799535,"user_tz":420,"elapsed":5,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}},"outputId":"8ae29fa7-1301-4c50-b582-5c80679407af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SubclassModel(\n","  (hidden_linear): Linear(in_features=1, out_features=13, bias=True)\n","  (hidden_activation): Tanh()\n","  (output_linear): Linear(in_features=13, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":2}],"source":["class SubclassModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()  # <1>\n","        self.hidden_linear = nn.Linear(1, 13)\n","        self.hidden_activation = nn.Tanh()\n","        self.output_linear = nn.Linear(13, 1)\n","\n","    def forward(self, input):\n","        hidden_t = self.hidden_linear(input)\n","        activated_t = self.hidden_activation(hidden_t)\n","        output_t = self.output_linear(activated_t)\n","        return output_t\n","\n","subclass_model = SubclassModel()\n","subclass_model"]},{"cell_type":"code","execution_count":3,"id":"66c0a579-6712-467a-83e6-daad865ec25f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66c0a579-6712-467a-83e6-daad865ec25f","executionInfo":{"status":"ok","timestamp":1698012799536,"user_tz":420,"elapsed":4,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}},"outputId":"7c24cdc1-a18b-4843-b37b-2bffc1fe083a"},"outputs":[{"output_type":"stream","name":"stdout","text":["t_u.shape --> torch.Size([11, 1])\n","t_un_train.shap --> torch.Size([9, 1])\n","t_un_val.shape --> torch.Size([2, 1])\n"]}],"source":["# prepare datasets\n","t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n","t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n","\n","# To accommodate multiple samples, modules expect the zeroth dimension of the input to be the number of samples in the batch\n","# we would need to add an extra dimension to turn that 1D tensor into a matrix with samples in the rows and features in the columns.\n","t_c = torch.tensor(t_c).unsqueeze(1) # Adds the extra dimension at axis 1\n","t_u = torch.tensor(t_u).unsqueeze(1) #\n","\n","print('t_u.shape -->', t_u.shape)\n","\n","n_samples = t_u.shape[0]\n","n_val = int(0.2 * n_samples)\n","\n","shuffled_indices = torch.randperm(n_samples)\n","\n","train_indices = shuffled_indices[:-n_val]\n","val_indices = shuffled_indices[-n_val:]\n","\n","t_u_train = t_u[train_indices]\n","t_c_train = t_c[train_indices]\n","\n","t_u_val = t_u[val_indices]\n","t_c_val = t_c[val_indices]\n","\n","t_un_train = 0.1 * t_u_train\n","t_un_val = 0.1 * t_u_val\n","\n","print('t_un_train.shap -->', t_un_train.shape)\n","print('t_un_val.shape -->', t_un_val.shape)"]},{"cell_type":"code","execution_count":4,"id":"944526f7-22da-446c-99d1-7000caeda346","metadata":{"id":"944526f7-22da-446c-99d1-7000caeda346","executionInfo":{"status":"ok","timestamp":1698012799746,"user_tz":420,"elapsed":212,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}}},"outputs":[],"source":["def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,\n","                  t_c_train, t_c_val):\n","    for epoch in range(1, n_epochs + 1):\n","        t_p_train = model(t_u_train) # <1>\n","        loss_train = loss_fn(t_p_train, t_c_train)\n","\n","        with torch.no_grad():\n","          t_p_val = model(t_u_val) # <1>\n","          loss_val = loss_fn(t_p_val, t_c_val)\n","\n","        optimizer.zero_grad()\n","        loss_train.backward() # <2>\n","        optimizer.step()\n","\n","        if epoch == 1 or epoch % 1000 == 0:\n","            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n","                  f\" Validation loss {loss_val.item():.4f}\")"]},{"cell_type":"code","execution_count":6,"id":"7d1b33df-9f6b-4572-9852-7b0fcbb6fd35","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7d1b33df-9f6b-4572-9852-7b0fcbb6fd35","executionInfo":{"status":"ok","timestamp":1698012825613,"user_tz":420,"elapsed":1363,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}},"outputId":"240bd6df-b9f7-414e-bb93-3f42d42386f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Training loss 3.9689, Validation loss 25.5023\n","Epoch 1000, Training loss 52.3371, Validation loss 225.1799\n","Epoch 2000, Training loss 34.1107, Validation loss 214.7565\n","Epoch 3000, Training loss 52.3920, Validation loss 226.8432\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[('hidden_linear.weight',\n","  Parameter containing:\n","  tensor([[-0.0605],\n","          [ 3.1941],\n","          [ 4.8323],\n","          [-4.6373],\n","          [-0.3479],\n","          [ 0.5380],\n","          [-3.2319],\n","          [ 1.1431],\n","          [-1.3278],\n","          [-0.4167],\n","          [ 3.5310],\n","          [-1.7390],\n","          [ 1.2383]], requires_grad=True)),\n"," ('hidden_linear.bias',\n","  Parameter containing:\n","  tensor([ -4.7709,  -4.3739,  -2.2809,   1.4888,   9.7830,  -9.8573,   0.8267,\n","          -18.4324,  18.9109,  -4.1175,  -1.0677,   1.6804,   1.8900],\n","         requires_grad=True)),\n"," ('output_linear.weight',\n","  Parameter containing:\n","  tensor([[-1.2746,  5.1810,  5.7557, -5.4474, -2.1109,  0.7308, -4.6444,  6.9543,\n","           -7.2767,  0.9431,  4.5173, -2.8148, -2.5398]], requires_grad=True)),\n"," ('output_linear.bias',\n","  Parameter containing:\n","  tensor([4.1430], requires_grad=True))]"]},"metadata":{},"execution_count":6}],"source":["optimizer = optim.SGD(subclass_model.parameters(), lr=1e-2)\n","\n","training_loop(\n","    n_epochs = 3000,\n","    optimizer = optimizer,\n","    model = subclass_model,\n","    loss_fn = nn.MSELoss(), # note: we are no longer using our hand-written loss function from earlier\n","    t_u_train = t_un_train,\n","    t_u_val = t_un_val,\n","    t_c_train = t_c_train,\n","    t_c_val = t_c_val)\n","\n","print()\n","\n","list(subclass_model.named_parameters())"]},{"cell_type":"code","execution_count":5,"id":"8f7d002a-5943-4753-9d4d-0eae0f4ecae0","metadata":{"id":"8f7d002a-5943-4753-9d4d-0eae0f4ecae0","executionInfo":{"status":"ok","timestamp":1698012801516,"user_tz":420,"elapsed":3,"user":{"displayName":"Hao Ji","userId":"00901522340792879006"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}